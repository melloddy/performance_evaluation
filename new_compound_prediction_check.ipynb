{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Sparsechem prediction for compounds with known labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied form Sparsechem utils.py\n",
    "def all_metrics(y_true, y_score):\n",
    "    y_classes = np.where(y_score > 0.5, 1, 0) \n",
    "    if len(y_true) <= 1:\n",
    "        df = pd.DataFrame({\"roc_auc_score\": [np.nan], \"auc_pr\": [np.nan], \"avg_prec_score\": [np.nan], \"max_f1_score\": [np.nan], \"kappa\": [np.nan]})\n",
    "        return df\n",
    "    if (y_true[0] == y_true).all():\n",
    "        df = pd.DataFrame({\"roc_auc_score\": [np.nan], \"auc_pr\": [np.nan], \"avg_prec_score\": [np.nan], \"max_f1_score\": [np.nan], \"kappa\": [np.nan]})\n",
    "        return df\n",
    "    roc_auc_score = sklearn.metrics.roc_auc_score(\n",
    "          y_true  = y_true,\n",
    "          y_score = y_score)\n",
    "    precision, recall, thresholds = sklearn.metrics.precision_recall_curve(y_true = y_true, probas_pred = y_score)\n",
    "\n",
    "    ## calculating F1 for all cutoffs\n",
    "    F1_score       = np.zeros(len(precision))\n",
    "    mask           = precision > 0\n",
    "    F1_score[mask] = 2 * (precision[mask] * recall[mask]) / (precision[mask] + recall[mask])\n",
    "\n",
    "    max_f1_score = F1_score.max()\n",
    "    auc_pr = sklearn.metrics.auc(x = recall, y = precision)\n",
    "    avg_prec_score = sklearn.metrics.average_precision_score(\n",
    "          y_true  = y_true,\n",
    "          y_score = y_score)\n",
    "    kappa = sklearn.metrics.cohen_kappa_score(y_true, y_classes)\n",
    "    df = pd.DataFrame({\"roc_auc_score\": [roc_auc_score], \"auc_pr\": [auc_pr], \"avg_prec_score\": [avg_prec_score], \"max_f1_score\": [max_f1_score], \"kappa\": [kappa]})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Sparsechem prediction (e.g. for new compounds or random subset of training set)\n",
    "pred = np.load(r\"y_hat.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate DataFrame with predictions and continuous task/compound IDs\n",
    "d = []\n",
    "cpd_counter = 0\n",
    "for row in pred:\n",
    "    task_counter = 0\n",
    "    for col in row:\n",
    "        d.append({\"cont_descriptor_vector_id\" : cpd_counter, \"cont_classification_task_id\" : task_counter, \"prediction\" : col})\n",
    "        task_counter += 1\n",
    "    cpd_counter += 1\n",
    "    \n",
    "df = pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get input_compound_ids from prediction mapping table\n",
    "T5_pred = pd.read_csv(r\"results_tmp\\T2_pred_mapping_table_T5.csv\")\n",
    "T5_pred.drop_duplicates(subset=\"input_compound_id\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge input ids with continuous ids from T11 prediction file\n",
    "T11_pred = pd.read_csv(r\"results\\T2_pred_T11.csv\")\n",
    "df2 = T5_pred.merge(T11_pred, on=\"descriptor_vector_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add ids to data frame with predictions\n",
    "df = df.merge(df2, on=\"cont_descriptor_vector_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mapping table from model training to get initial task and assay IDs\n",
    "T3_train = pd.read_csv(r\"training\\results\\weight_table_T3_mapped.csv\")\n",
    "df = df.merge(T3_train, on=\"cont_classification_task_id\")\n",
    "df = df.drop(\"weight\", 1)\n",
    "df = df.drop(\"assay_type\", 1)\n",
    "df = df.drop(\"fp_val_json\", 1)\n",
    "df = df.drop(\"fp_json\", 1)\n",
    "df = df.drop(\"fold_id\", 1)\n",
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with input labels based on task and compound ids (T4-like file needs to be prepared for the prediction compounds)\n",
    "act = pd.read_csv(r\"T4_like.csv\")\n",
    "act = act.groupby([\"input_compound_id\", \"classification_task_id\"]).agg(lambda x:x.value_counts().index[0])\n",
    "df = df.merge(act, on=[\"input_compound_id\", \"classification_task_id\"])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check predictions for input 1s (most should be close to 1)\n",
    "df_actives = df[df[\"class_label\"] == 1]\n",
    "df_actives.drop([\"cont_classification_task_id\", \"classification_task_id\"], 1, inplace=True)\n",
    "print(df_actives.sort_values(\"prediction\", ascending = False).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview df\n",
    "df.sort_values([\"input_compound_id\", \"classification_task_id\"]).head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics over all tasks\n",
    "all_metrics(df['class_label'], df['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check scores for individual assays or tasks\n",
    "for assay in df.input_assay_id.unique():\n",
    "    assay_pred = df[df['input_assay_id'] == assay].reset_index()\n",
    "    print(assay)\n",
    "    print(all_metrics(assay_pred['class_label'], assay_pred['prediction']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
