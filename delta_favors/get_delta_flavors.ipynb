{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4654354c-0f54-4c2d-82b8-9fc8718c504b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7706e5ef-d590-4d4e-8deb-f5d27e4c1734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in these pathes\n",
    "\n",
    "# WP3 performance_evaluation.py output folders \n",
    "perf_eval_outputs = {\n",
    "    \"cls\" : \"path to performance_evaluation.py output folder of CLS MP-SP\",\n",
    "    \"clsaux\" : \"path to performance_evaluation.py output folder of CLSAUX MP-SP\",\n",
    "    \"reg\" : \"path to performance_evaluation.py output folder of REG MP-SP\",\n",
    "    \"hyb\" : \"path to performance_evaluation.py output folder of HYB MP-SP\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87453e31-b9f1-4839-8aea-87017560136f",
   "metadata": {},
   "source": [
    "# Delta relative to baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cf8457-38e5-4845-be8e-7d032c9db34e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cls_metrics = ['roc_auc_score', 'auc_pr', 'auc_pr_cal']\n",
    "reg_metrics = ['rsquared', 'corrcoef', 'rmse_uncen', 'rmse']\n",
    "\n",
    "assay_type_reg = {'OTHER':'OTHER',\n",
    "                  'ADME':'ADME',\n",
    "                  'NON-CATALOG-PANEL':'PANEL',\n",
    "                  'CATALOG-PANEL':'PANEL'}\n",
    "\n",
    "\n",
    "for dataset, path in perf_eval_outputs.items():\n",
    "    columns = ['input_assay_id', 'assay_type']\n",
    "    \n",
    "    metrics = cls_metrics\n",
    "    task_id_col = 'cont_classification_task_id'\n",
    "    subfolders = [dataset]\n",
    "    \n",
    "    if dataset in ['reg', 'hyb']:\n",
    "        subfolders=['regr', 'regr_cens']\n",
    "        task_id_col = 'cont_regression_task_id'\n",
    "        metrics = reg_metrics\n",
    "    \n",
    "    columns += metrics\n",
    "    columns.append(task_id_col)\n",
    "    \n",
    "    for subdir in subfolders:\n",
    "        print(f\"{dataset:<7} {os.path.join(path, subdir)}\")\n",
    "        perf_eval_outdir = os.path.join(path, subdir)\n",
    "        delta_outdir = os.path.join(perf_eval_outdir, 'deltas_relative_baseline')\n",
    "\n",
    "        os.makedirs(delta_outdir, exist_ok=True)\n",
    "\n",
    "        sp_perf_file = glob.glob(os.path.join(perf_eval_outdir, 'SP', \"*per-task_performances_NOUPLOAD.csv\"))\n",
    "        mp_perf_file = glob.glob(os.path.join(perf_eval_outdir, 'MP', \"*per-task_performances_NOUPLOAD.csv\"))\n",
    "\n",
    "        assert not len(sp_perf_file) == 0, f\"Cannot find the SP task based performance file under {os.path.join(perf_eval_outdir, 'SP')}\"\n",
    "        assert not len(mp_perf_file) == 0, f\"Cannot find the MP task based performance file under {os.path.join(perf_eval_outdir, 'MP')}\"\n",
    "\n",
    "        assert not len(sp_perf_file) > 1, f\"There is more than one task based performance file under {os.path.join(perf_eval_outdir, 'SP', '*per-task_performances_NOUPLOAD.csv')}\"\n",
    "        assert not len(mp_perf_file) > 1, f\"There is more than one task based performance file under {os.path.join(perf_eval_outdir, 'MP', '*per-task_performances_NOUPLOAD.csv')}\"\n",
    "\n",
    "        sp_perf = pd.read_csv(sp_perf_file[0], usecols=columns)\n",
    "        mp_perf = pd.read_csv(mp_perf_file[0], usecols=columns)\n",
    "\n",
    "        merged = mp_perf.merge(sp_perf, on=[task_id_col, 'input_assay_id', 'assay_type'], suffixes=('_mp', '_sp'))\n",
    "        if dataset in ['reg','hyb']:\n",
    "            merged['assay_type'] = merged['assay_type'].map(assay_type_reg)\n",
    "\n",
    "        for m in metrics:\n",
    "            if f'{m}_mp' not in merged.columns:continue\n",
    "            merged[f'{m}'] = (merged[f'{m}_mp'] - merged[f'{m}_sp']) / merged[f'{m}_sp']\n",
    "\n",
    "        merged.to_csv(os.path.join(delta_outdir, 'deltas_per-task_performances_NOUPLOAD.csv'), index=None)\n",
    "\n",
    "        means = merged[metrics].mean()\n",
    "        delta_global = pd.DataFrame([means.values], columns=means.index)\n",
    "        delta_global.to_csv(os.path.join(delta_outdir, 'deltas_global_performances.csv'), index=None)\n",
    "\n",
    "        delta_assay_type = merged.groupby('assay_type').mean()[metrics].reset_index()\n",
    "        delta_assay_type.to_csv(os.path.join(delta_outdir, 'deltas_per-assay_performances.csv'), index=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa664c4-72e9-4836-a075-279fc925d372",
   "metadata": {},
   "source": [
    "# Delta relative to perfection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9e0743-ae10-495c-97b5-20f6ccb41adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_metrics = ['roc_auc_score', 'auc_pr', 'auc_pr_cal']\n",
    "reg_metrics = ['rsquared', 'corrcoef', 'rmse_uncen', 'rmse']\n",
    "\n",
    "\n",
    "assay_type_reg = {'OTHER':'OTHER',\n",
    "                  'ADME':'ADME',\n",
    "                  'NON-CATALOG-PANEL':'PANEL',\n",
    "                  'CATALOG-PANEL':'PANEL'}\n",
    "\n",
    "\n",
    "for dataset, path in perf_eval_outputs.items():\n",
    "    columns = ['input_assay_id', 'assay_type']\n",
    "    \n",
    "    metrics = cls_metrics\n",
    "    task_id_col = 'cont_classification_task_id'\n",
    "    subfolders = [dataset]\n",
    "    \n",
    "    if dataset in ['reg', 'hyb']:\n",
    "        subfolders=['regr', 'regr_cens']\n",
    "        task_id_col = 'cont_regression_task_id'\n",
    "        metrics = reg_metrics\n",
    "    \n",
    "    columns += metrics\n",
    "    columns.append(task_id_col)\n",
    "    \n",
    "    for subdir in subfolders:\n",
    "        print(f\"{dataset:<7} {os.path.join(path, subdir)}\")\n",
    "        perf_eval_outdir = os.path.join(path, subdir)\n",
    "        delta_outdir = os.path.join(perf_eval_outdir, 'deltas_relative_perfection')\n",
    "\n",
    "        os.makedirs(delta_outdir, exist_ok=True)\n",
    "\n",
    "        sp_perf_file = glob.glob(os.path.join(perf_eval_outdir, 'SP', \"*per-task_performances_NOUPLOAD.csv\"))\n",
    "        mp_perf_file = glob.glob(os.path.join(perf_eval_outdir, 'MP', \"*per-task_performances_NOUPLOAD.csv\"))\n",
    "\n",
    "        assert not len(sp_perf_file) == 0, f\"Cannot find the SP task based performance file under {os.path.join(perf_eval_outdir, 'SP')}\"\n",
    "        assert not len(mp_perf_file) == 0, f\"Cannot find the MP task based performance file under {os.path.join(perf_eval_outdir, 'MP')}\"\n",
    "\n",
    "        assert not len(sp_perf_file) > 1, f\"There is more than one task based performance file under {os.path.join(perf_eval_outdir, 'SP', '*per-task_performances_NOUPLOAD.csv')}\"\n",
    "        assert not len(mp_perf_file) > 1, f\"There is more than one task based performance file under {os.path.join(perf_eval_outdir, 'MP', '*per-task_performances_NOUPLOAD.csv')}\"\n",
    "\n",
    "        sp_perf = pd.read_csv(sp_perf_file[0], usecols=columns)\n",
    "        mp_perf = pd.read_csv(mp_perf_file[0], usecols=columns)\n",
    "\n",
    "        merged = mp_perf.merge(sp_perf, on=[task_id_col, 'input_assay_id', 'assay_type'], suffixes=('_mp', '_sp'))\n",
    "        if dataset in ['reg', 'hyb']:\n",
    "            merged['assay_type'] = merged['assay_type'].map(assay_type_reg)\n",
    "            \n",
    "        for m in metrics:\n",
    "            assert f'{m}_mp' in merged.columns, f\"Didn't find {m} in task level performance\"\n",
    "\n",
    "            max_performance = 1\n",
    "            if 'rmse' in m:\n",
    "                max_performance = 0\n",
    "\n",
    "            merged[f'{m}'] = (merged[f'{m}_mp'] - merged[f'{m}_sp']) / (max_performance - merged[f'{m}_sp'])\n",
    "\n",
    "        merged.to_csv(os.path.join(delta_outdir, 'deltas_per-task_performances_NOUPLOAD.csv'), index=None)\n",
    "\n",
    "        means = merged[metrics].mean()\n",
    "        delta_global = pd.DataFrame([means.values], columns=means.index)\n",
    "        delta_global.to_csv(os.path.join(delta_outdir, 'deltas_global_performances.csv'), index=None)\n",
    "\n",
    "        delta_assay_type = merged.groupby('assay_type').mean()[metrics].reset_index()\n",
    "        delta_assay_type.to_csv(os.path.join(delta_outdir, 'deltas_per-assay_performances.csv'), index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c8e4d5-615d-4df3-aec0-6205fbd08bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged['assay_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ac6749-733f-4b4f-90bd-c24b7b131438",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Melloddy Pipeline (y3)",
   "language": "python",
   "name": "melloddy_pipeline"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
